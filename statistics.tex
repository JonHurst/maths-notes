\documentclass[a5paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{bm}
\usepackage{geometry}
\usepackage{graphics}

\title{Statistics}
\author{Jon Hurst}

\begin{document}
\maketitle

\section{Mean, Variance and Standard Deviation}

If $f_i$ is the frequency that value $x_i$ occurs and $\overline{x}$ and
$\sigma^2$ are respectively the mean and variance for the frequency
distribution, then

\begin{equation}
  \overline{x} = \frac{\sum\limits_{all\ i} f_i x_i}{\sum\limits_{all\ i} f_i}
\end{equation}

\begin{equation} \label{eq:1}
  \sigma^2 = \frac{\sum\limits_{all\ i} f_i (x_i - \overline{x})^2}{\sum\limits_{all\ i} f_i}
\end{equation}

Since

\begin{equation}
  \frac{\sum\limits_{all\ i} f_i (x_i - \overline{x})^2}{\sum\limits_{all\ i} f_i} =
  \frac{\sum\limits_{all\ i} f_i x_i^2}{\sum\limits_{all\ i} f_i}
  -2\overline{x}\ \frac{\sum\limits_{all\ i} f_i x_i}{\sum\limits_{all\ i} f_i}
  + \overline{x}\ \frac{\sum\limits_{all\ i} f_i}{\sum\limits_{all\ i} f_i}
\end{equation}

An alternative version of (\ref{eq:1}) is

\begin{equation} \label{eq:2}
  \sigma^2 = \frac{\sum\limits_{all\ i} f_i x_i^2}{\sum\limits_{all\ i} f_i} -
  \overline{x}^2
\end{equation}

(\ref{eq:2}) is sometimes verbalised as ``the mean of the squares minus the square
of the means''.

Standard deviation, $\sigma$, is the square root of the variance. The advantage
of the standard deviation is that it has the same units as the mean.

\section{Weighted Mean}

The concept of a weighted mean is that some values are considered to be ``more
important'' to the mean than others. If we had, for example, a set of integer
values between 1 and 10 and we wanted 7 to be twice as important to the mean as
the other numbers, if each time 7 occurs we treat it as having occurred twice we
achieve the desired effect. In general, if we assign a weight, $w_i$, to each
possible value, then

\begin{equation}
  \overline{x} = \frac{\sum\limits_{all\ i} w_i f_i x_i}{\sum\limits_{all\ i}
    w_i f_i}
\end{equation}

Considering the above example, $w_i$ could be $1$ except for $w_7$ which would
be $2$. If $w_i$ were $\frac{1}{11}$ except for $w_7$ which would be
$\frac{2}{11}$, the result is the same because the $\frac{1}{11}$ factor can be
taken outside the summation signs for both numerator and denominator. The latter
scheme allows the total weight to add to 1, which is often how weights are
specified.

\section{Arrangements}

Consider all the ways that the numbers in the set \{1,2,3,4\} could be ordered.
One way to do this methodically would be with a tree diagram, as shown if Figure
\ref{fig:1}. The total number of possible arrangements is the number of entries
on the bottom line, and this can be seen to be $4\times 3\times 2\times 1 = 24$.
Essentially, there are 4 ways to fill the first number, then for each of those
4, 3 ways to fill the second number, then for each of those 3 only 2 ways to
fill the third number, with the last number being the only one remaining.

\begin{figure}[ht]

  \begin{center}
    \includegraphics{arrangements}
  \end{center}
  \caption{Arrangements tree}
  \label{fig:1}
\end{figure}

This process can be generalised for $n$ numbers. The number of arrangements of
$n$ numbers will result from there being $n$ ways to fill the first number, then
for each of those $n$ ways, $(n-1)$ ways to fill the second number, continuing
until there is only one option for the final number. If $A_n$ is the number of
possible arrangements of $n$ numbers, therefore

\begin{equation} \label{eq:3}
  A_n = n!
\end{equation}

These numbers can, of course, represent any set of $n$ distinct objects -- if
they were physical objects you could always write the numbers on sticky labels
and attach them to the objects.

\section{Groupings of arrangements}

Equation (\ref{eq:3}) gives the number of arrangements of distinct objects, but
what if some of the objects are indistinguishable from one another, e.g. you
wish to know how many arrangements can be made of 3 black balls and 2 white
balls, given that you can't tell the difference between balls of the same
colour?

The trick is to first label the objects, so that you \textit{can} tell the
difference, then group together the arrangements that \textit{would} be
indistinguishable without the labels. The answer is then the number of groups,
which can be calculated by dividing the total possible arrangements by the size
of a group.

In the example there are $5!$ ways of arranging the labelled balls, and each
grouping includes the $3!$ ways the black balls could be arranged and the $2!$
ways the white balls could be arranged that would result in indistinguishable
results (see Figure \ref{fig:2}).

\begin{figure}[ht]
  \begin{center}
    \includegraphics{arrangement-grouping}
  \end{center}

  \caption{Arrangement grouping}
  \label{fig:2}
\end{figure}

Thus the size of each group is $3!2!$, and the number of arrangements, given
that you can't tell the difference between balls of the same colour, is
$5!/(3!2!) = 10$.

\section{Rings}

If $n$ objects are arranged in a ring, and the orientation of the ring is
considered to be unimportant, then the size of the groupings of
indistinguishable arrangements is equal to the number of times the ring can be
rotated to give a new arrangement before it ends up back at the start. If there
are $n$ objects on the ring, therefore, the size of the groupings is $n$.
Hence, if $R_n$ is the number of distinguishable arrangements on a ring

\begin{equation}
  R_n = \frac{n!}{n} = (n-1)!
\end{equation}

If the ring may also be flipped, with all possible rotated arrangements in the
flipped state also to be considered indistinguishable, then the size of the
groupings is $2n$, and the number of distinguishable arrangements is $R_n / 2$.

\section{Permutations and Combinations}

The number of permutations, ${}^nP_r$, is the number of possible arrangements
resulting from selecting $r$ objects from a set of $n$ objects. The number of
ways of arranging the $n$ objects is $n!$, but only the first $r$ objects are
distinguishable. This means that the arrangements can be grouped by
considering all sub-arrangement of the remaining $(n-r)$ objects to be
indistinguishable, i.e. the group size is $(n-r)!$. Thus

\begin{equation}
  {}^nP_r = \frac{n!}{(n-r)!}
\end{equation}

The number of combinations, ${}^nC_r$, is the number of permutations but
considering order to be unimportant. This means the groupings of arrangements
include not only all possible sub-arrangements of the remaining $(n-r)$ objects,
but also all possible sub-arrangements of the $r$ selected objects. Thus the
group size is $r!(n-r)!$ and hence

\begin{equation}
  {}^nC_r = \frac{{}^nP_r}{r!} = \frac{n!}{(n-r)!r!}
\end{equation}

\section{Probability}
\subsection{Conditional probability}

$P(A\,|\,B)$ is the probability that an event A occurs given that an event B also
occurs. To calculate it, restrict the possibility space to only those outcomes
in which B occurs and consider how many times A also occurs.

\begin{equation} \label{eq:4}
  P(A\,|\,B) = \frac{P(A\cap B)}{P(B)}
\end{equation}

Since $P(A\cap B) = P(B\cap A)$ this also gives us

\begin{equation} \label{eq:5}
  P(A\,|\,B)P(B) = P(B\,|\,A)P(A)
\end{equation}
\subsection{Independent events}

Events A and B are independent if the occurrence of event B does not affect the
probability that A also occurs, i.e.

\begin{equation}
  P(A\,|\,B) = P(A)\hspace{3em} \mathrm{and}\hspace{3em} P(B\,|\,A) = P(B)
\end{equation}

Substituting equation (\ref{eq:4}) gives, \textit{for independent events only}

\begin{equation}
  P(A\cap B) = P(A)P(B)
\end{equation}

\subsection{Mutually exclusive events}

Events are mutually exclusive if

\begin{equation}
  P(A\cap B) = 0
\end{equation}

\subsection{Exhaustive events}

Events are exhaustive if

\begin{equation}
  P(A\cup B) = 1
\end{equation}

\subsection{Non-occurrence}

If $P(A)$ is the probability that event A occurs, $P(A^\prime)$ is the
probability that A \textit{does not} occur. The following identities are useful

\begin{eqnarray}
  P(A) = P(A\cap B) + P(A\cap B^\prime) \\
  P(B) = P(B\cap A) + P(B\cap A^\prime) \\
  P(A^\prime \cap B^\prime) = 1 - P(A \cup B)
\end{eqnarray}

\subsection{Bayes Theorem}

Rearranging equation (\ref{eq:5}) gives Bayes Theorem for two events

\begin{equation}
  P(A\,|\,B) = \frac{P(B\,|\,A)\ P(A)}{P(B)}
\end{equation}

\noindent If $A_i, i = 1, 2, 3, \ldots, n$ are $n$ exhaustive events then

\begin{equation}
  P(B) = \sum_{i=1}^{n} P(B\cap A_i)
\end{equation}

\begin{equation}
  \Rightarrow P(A_i\,|\,B) = \frac{P(B\,|\,A_i) P(A_i)}{\sum\limits_{j=1}^{n}P(B\,|\,A_j) P(A_j)}
\end{equation}

\begin{figure}[ht]
  \includegraphics{bayes-theorem}

  \caption{Bayes Theorem}
  \label{fig:3}
\end{figure}

\noindent Figure \ref{fig:3} shows this relationship on a tree diagram.


\end{document}
